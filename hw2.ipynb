{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 2\n",
    "\n",
    "In this homework, you will be implementing a neural network library in the needle framework. Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Code to set up the assignment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "!git clone https://github.com/dlsys10714/hw2.git\n",
    "%cd /content/drive/MyDrive/10714/hw2\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 0\n",
    "\n",
    "This homework builds off of Homework 1. First, in your Homework 2 directory, copy the files `python/needle/autograd.py`, `python/needle/ops/ops_mathematic.py` from your Homework 1.\n",
    "\n",
    "***NOTE***: The default data type for the tensor is `float32`. If you want to change the data type, you can do so by setting the `dtype` parameter in the `Tensor` constructor. For example, `Tensor([1, 2, 3], dtype='float64')` will create a tensor with `float64` data type. \n",
    "In this homework, make sure any tensor you create has `float32` data type to avoid any issues with the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:14.160020Z",
     "start_time": "2024-07-24T14:37:14.157614Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(*shape, entropy=1):\n",
    "    np.random.seed(np.prod(shape) * len(shape) * entropy)\n",
    "    return ndl.Tensor(np.random.randint(0, 100, size=shape) / 20, dtype=\"float32\")\n",
    "\n",
    "def linear_backward(lhs_shape, rhs_shape):\n",
    "    np.random.seed(199)\n",
    "    f = ndl.nn.Linear(*lhs_shape)\n",
    "    f.bias.data = get_tensor(lhs_shape[-1])\n",
    "    x = get_tensor(*rhs_shape)\n",
    "    (f(x) ** 2).sum().backward()\n",
    "    return x.grad.cached_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T01:23:14.248587Z",
     "start_time": "2024-07-25T01:23:14.172996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<needle.ops.ops_mathematic.Summation object at 0x118af3970>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x118af3760>\n",
      "(1, 5)\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x118af36a0>\n",
      "(1, 5)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x118af32b0>\n",
      "(1, 5)\n",
      "(0, 1)\n",
      "[[ 7.699897  19.00998    4.76058   11.145172  11.3367815]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlinear_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mlinear_backward\u001b[0;34m(lhs_shape, rhs_shape)\u001b[0m\n\u001b[1;32m      8\u001b[0m f\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m get_tensor(lhs_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m get_tensor(\u001b[38;5;241m*\u001b[39mrhs_shape)\n\u001b[0;32m---> 10\u001b[0m \u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mcached_data\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/autograd.py:297\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, out_grad)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, out_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    292\u001b[0m     out_grad \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    293\u001b[0m         out_grad\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out_grad\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m init\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    296\u001b[0m     )\n\u001b[0;32m--> 297\u001b[0m     \u001b[43mcompute_gradient_of_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/autograd.py:390\u001b[0m, in \u001b[0;36mcompute_gradient_of_variables\u001b[0;34m(output_tensor, out_grad)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mprint\u001b[39m(node\u001b[38;5;241m.\u001b[39mop)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mprint\u001b[39m(node\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 390\u001b[0m input_grads \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_as_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_grads, Tuple):\n\u001b[1;32m    392\u001b[0m     input_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(input_grads)\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/autograd.py:67\u001b[0m, in \u001b[0;36mOp.gradient_as_tuple\u001b[0;34m(self, out_grad, node)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient_as_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, out_grad: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, node: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience method to always return a tuple from gradient call\"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/ops/ops_mathematic.py:222\u001b[0m, in \u001b[0;36mBroadcastTo.gradient\u001b[0;34m(self, out_grad, node)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mprint\u001b[39m(axis_to_sum)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mprint\u001b[39m(out_grad)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_to_sum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/ops/ops_mathematic.py:207\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, shape):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/autograd.py:80\u001b[0m, in \u001b[0;36mTensorOp.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_from_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/autograd.py:242\u001b[0m, in \u001b[0;36mTensor.make_from_op\u001b[0;34m(op, inputs)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 242\u001b[0m     \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealize_cached_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/autograd.py:107\u001b[0m, in \u001b[0;36mValue.realize_cached_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_data\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# note: data implicitly calls realized cached data\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealize_cached_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_data\n",
      "File \u001b[0;32m~/Documents/GitHub/dlsyscourse/DLS-hw2/./python/needle/ops/ops_mathematic.py:197\u001b[0m, in \u001b[0;36mReshape.compute\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m### BEGIN YOUR SOLUTION\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/core/fromnumeric.py:285\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (5,)"
     ]
    }
   ],
   "source": [
    "linear_backward((10, 5), (1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T01:23:15.056103Z",
     "start_time": "2024-07-25T01:23:15.047996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_grad: (1, 3, 5), in_shape: (5,)\n",
      "singleton: [0, 1]\n",
      "summation: (5,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 16.318823  ,   0.38907146,  -2.3196597 , -10.607947  ,\n",
       "          -8.891978  ,  16.04581   ,   9.475689  ,  14.571134  ,\n",
       "           6.581476  ,  10.204643  ],\n",
       "        [ 20.291656  ,   7.48733   ,   1.2581359 , -14.285495  ,\n",
       "          -6.0252004 ,  19.621626  ,   4.3433027 ,   6.9732003 ,\n",
       "          -0.81034946,   4.037069  ],\n",
       "        [ 11.332954  ,  -5.6982875 ,  -8.81556   ,  -7.673438  ,\n",
       "          -7.616167  ,   9.361553  ,  17.341637  ,  17.269142  ,\n",
       "          18.107597  ,  14.261493  ]]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_backward((10, 5), (1, 3, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1\n",
    "\n",
    "In this first question, you will implement a few different methods for weight initialization.  This will be done in the `python/needle/init/init_initializers.py` file, which contains a number of routines for initializing needle Tensors using various random and constant initializations.  Following the same methodology of the existing initializers (you will want to call e.g. `init.rand` or `init.randn` implemented in `python/needle/init/init_basic.py` from your functions below, implement the following common initialization methods.  In all cases, the functions should return `fan_in` by `fan_out` 2D tensors (extensions to other sizes can be done via e.g., reshaping).\n",
    "\n",
    "\n",
    "### Xavier uniform\n",
    "`xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{U}(-a, a)$ where \n",
    "\\begin{equation}\n",
    "a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan_in} + \\text{fan_out}}}\n",
    "\\end{equation}\n",
    "\n",
    "Pass remaining `**kwargs` parameters to the corresponding `init` random call.\n",
    "\n",
    "##### Parameters\n",
    "- `fan_in` - dimensionality of input\n",
    "- `fan_out` - dimensionality of output\n",
    "- `gain` - optional scaling factor\n",
    "___\n",
    "\n",
    "### Xavier normal\n",
    "`xavier_normal(fan_in, fan_out, gain=1.0, **kwargs)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a normal distribution. The resulting Tensor will have values sampled from $\\mathcal{N}(0, \\text{std}^2)$ where \n",
    "\\begin{equation}\n",
    "\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan_in} + \\text{fan_out}}}\n",
    "\\end{equation}\n",
    "\n",
    "##### Parameters\n",
    "- `fan_in` - dimensionality of input\n",
    "- `fan_out` - dimensionality of output\n",
    "- `gain` - optional scaling factor\n",
    "___\n",
    "\n",
    "### Kaiming uniform\n",
    "`kaiming_uniform(fan_in, fan_out, nonlinearity=\"relu\", **kwargs)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{U}(-\\text{bound}, \\text{bound})$ where \n",
    "\\begin{equation}\n",
    "\\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan_in}}}\n",
    "\\end{equation}\n",
    "\n",
    "Use the recommended gain value for ReLU: $\\text{gain}=\\sqrt{2}$.\n",
    "\n",
    "##### Parameters\n",
    "- `fan_in` - dimensionality of input\n",
    "- `fan_out` - dimensionality of output\n",
    "- `nonlinearity` - the non-linear function\n",
    "___\n",
    "\n",
    "### Kaiming normal\n",
    "`kaiming_normal(fan_in, fan_out, nonlinearity=\"relu\", **kwargs)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{N}(0, \\text{std}^2)$ where \n",
    "\\begin{equation}\n",
    "\\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan_in}}}\n",
    "\\end{equation}\n",
    "\n",
    "Use the recommended gain value for ReLU: $\\text{gain}=\\sqrt{2}$.\n",
    "\n",
    "##### Parameters\n",
    "- `fan_in` - dimensionality of input\n",
    "- `fan_out` - dimensionality of output\n",
    "- `nonlinearity` - the non-linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:37:18.613523Z",
     "start_time": "2024-07-24T14:37:18.094926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.11.9, pytest-7.4.0, pluggy-1.0.0 -- /home/longcxiang/anaconda3/envs/needle/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/longcxiang/Documents/DLS/DLS-hw2\r\n",
      "plugins: anyio-4.2.0\r\n",
      "collected 93 items / 89 deselected / 4 selected                                \u001b[0m\r\n",
      "\r\n",
      "tests/hw2/test_nn_and_optim.py::test_init_kaiming_uniform \u001b[32mPASSED\u001b[0m\u001b[32m         [ 25%]\u001b[0m\r\n",
      "tests/hw2/test_nn_and_optim.py::test_init_kaiming_normal \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\r\n",
      "tests/hw2/test_nn_and_optim.py::test_init_xavier_uniform \u001b[32mPASSED\u001b[0m\u001b[32m          [ 75%]\u001b[0m\r\n",
      "tests/hw2/test_nn_and_optim.py::test_init_xavier_normal \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m89 deselected\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m =======================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_init\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"init\" -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "In this question, you will implement additional modules in `python/needle/nn/nn_basic.py`. Specifically, for the following modules described below, initialize any variables of the module in the constructor, and fill out the `forward` method. **Note:** Be sure that you are using the `init` functions that you just implemented to initialize the parameters, and don't forget to pass the `dtype` argument.\n",
    "___\n",
    "\n",
    "### Linear\n",
    "`needle.nn.Linear(in_features, out_features, bias=True, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$. The input shape is $(N, H_{in})$ where $H_{in}=\\text{in_features}$. The output shape is $(N, H_{out})$ where $H_{out}=\\text{out_features}$.\n",
    "\n",
    "Be careful to explicitly broadcast the bias term to the correct shape -- Needle does not support implicit broadcasting.\n",
    "\n",
    "**Note:** for all layers including this one, you should initialize the weight Tensor before the bias Tensor, and should initialize all Parameters using only functions from `init`.\n",
    "\n",
    "##### Parameters\n",
    "- `in_features` - size of each input sample\n",
    "- `out_features` - size of each output sample\n",
    "- `bias` - If set to `False`, the layer will not learn an additive bias.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of shape (`in_features`, `out_features`). The values should be initialized with the Kaiming Uniform initialization with `fan_in = in_features`\n",
    "- `bias` - the learnable bias of shape (1, `out_features`). The values should be initialized with the Kaiming Uniform initialize with `fan_in = out_features`. **Note the difference in fan_in choice, due to their relative sizes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.13, pytest-8.3.2, pluggy-1.5.0 -- /Users/longxiangzhang/opt/miniconda3/envs/ml/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2\n",
      "plugins: anyio-4.0.0\n",
      "collected 93 items / 85 deselected / 8 selected                                \u001b[0m\n",
      "\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_linear_weight_init_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 12%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_linear_bias_init_1 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 25%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_linear_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 37%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_linear_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_linear_forward_3 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 62%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_linear_backward_1 \u001b[31mFAILED\u001b[0m\u001b[31m         [ 75%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_linear_backward_2 \u001b[31mFAILED\u001b[0m\u001b[31m         [ 87%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_linear_backward_3 \u001b[31mFAILED\u001b[0m\u001b[31m         [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ test_nn_linear_backward_1 ___________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_linear_backward_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           linear_backward((\u001b[94m10\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            np.array(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m20.61148\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m6.920893\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m1.625556\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m13.497676\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m6.672813\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m18.762121\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m7.286628\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m8.18535\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m2.741301\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m5.723689\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "                ],\u001b[90m\u001b[39;49;00m\n",
      "                dtype=np.float32,\u001b[90m\u001b[39;49;00m\n",
      "            ),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:797: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:156: in linear_backward\n",
      "    \u001b[0m(f(x) ** \u001b[94m2\u001b[39;49;00m).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in gradient\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m reshape(summation(out_grad, axis_to_sum), node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:207: in reshape\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:197: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:285: in reshape\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mreshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, newshape, order=order)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = 53.95241, method = 'reshape', args = ((5,),), kwds = {'order': 'C'}\n",
      "bound = <built-in method reshape of numpy.float32 object at 0x1035e0dd0>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapfunc\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        bound = \u001b[96mgetattr\u001b[39;49;00m(obj, method, \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m bound \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m bound(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           ValueError: cannot reshape array of size 1 into shape (5,)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:59: ValueError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x10e3583d0>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x10e358310>\n",
      "(1, 5)\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x10e3581c0>\n",
      "(1, 5)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x10e358130>\n",
      "(1, 5)\n",
      "(0, 1)\n",
      "[[ 7.699897  19.00998    4.76058   11.145172  11.3367815]]\n",
      "\u001b[31m\u001b[1m__________________________ test_nn_linear_backward_2 ___________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_linear_backward_2\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[96mprint\u001b[39;49;00m(linear_backward((\u001b[94m10\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m3\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m)))\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:821: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:156: in linear_backward\n",
      "    \u001b[0m(f(x) ** \u001b[94m2\u001b[39;49;00m).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in gradient\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m reshape(summation(out_grad, axis_to_sum), node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:207: in reshape\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:197: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:285: in reshape\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mreshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, newshape, order=order)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = 168.47995, method = 'reshape', args = ((5,),), kwds = {'order': 'C'}\n",
      "bound = <built-in method reshape of numpy.float32 object at 0x10f01de50>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapfunc\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        bound = \u001b[96mgetattr\u001b[39;49;00m(obj, method, \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m bound \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m bound(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           ValueError: cannot reshape array of size 1 into shape (5,)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:59: ValueError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x10e3fd180>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x10e3fd150>\n",
      "(3, 5)\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x10e3fd270>\n",
      "(3, 5)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x10e3fd480>\n",
      "(3, 5)\n",
      "(0, 1)\n",
      "[[15.526177   20.17357     0.76063204 12.485003   13.889327  ]\n",
      " [ 5.0965495  15.49585    10.68631     4.1313877  19.742485  ]\n",
      " [ 5.743393   14.932665    8.47385     4.9237947  16.418953  ]]\n",
      "\u001b[31m\u001b[1m__________________________ test_nn_linear_backward_3 ___________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_linear_backward_3\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[96mprint\u001b[39;49;00m(linear_backward((\u001b[94m10\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m)))\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:871: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:156: in linear_backward\n",
      "    \u001b[0m(f(x) ** \u001b[94m2\u001b[39;49;00m).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in gradient\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m reshape(summation(out_grad, axis_to_sum), node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:207: in reshape\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:197: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:285: in reshape\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mreshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, newshape, order=order)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = 154.50189, method = 'reshape', args = ((5,),), kwds = {'order': 'C'}\n",
      "bound = <built-in method reshape of numpy.float32 object at 0x10e5064f0>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapfunc\u001b[39;49;00m(obj, method, *args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        bound = \u001b[96mgetattr\u001b[39;49;00m(obj, method, \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m bound \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m bound(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           ValueError: cannot reshape array of size 1 into shape (5,)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/core/fromnumeric.py\u001b[0m:59: ValueError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x10f0b4eb0>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x10f0b4c40>\n",
      "(1, 3, 5)\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x10f0b4c70>\n",
      "(1, 3, 5)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x10f0b42e0>\n",
      "(1, 3, 5)\n",
      "(0, 1, 2)\n",
      "[[[ 8.702918  17.565617   7.8714223  6.0634203 16.028439 ]\n",
      "  [10.428917  17.457577   4.753628  11.344371   9.948637 ]\n",
      "  [ 2.686408  17.278755   5.2087183 -0.5659107 19.728996 ]]]\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_nn_linear_backward_1\u001b[0m - ValueError: cannot reshape array of size 1 into shape (5,)\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_nn_linear_backward_2\u001b[0m - ValueError: cannot reshape array of size 1 into shape (5,)\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_nn_linear_backward_3\u001b[0m - ValueError: cannot reshape array of size 1 into shape (5,)\n",
      "\u001b[31m================== \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m5 passed\u001b[0m, \u001b[33m85 deselected\u001b[0m\u001b[31m in 0.27s\u001b[0m\u001b[31m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "`needle.nn.ReLU()`\n",
    "\n",
    "Applies the rectified linear unit function element-wise:\n",
    "$ReLU(x) = max(0, x)$.\n",
    "\n",
    "If you have previously implemented ReLU's backwards pass in terms of itself, note that this is numerically unstable and will likely cause problems\n",
    "down the line.\n",
    "Instead, consider that we could write the derivative of ReLU as $I\\{x>0\\}$, where we arbitrarily decide that the derivative at $x=0$ is 0.\n",
    "(This is a _subdifferentiable_ function.)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(2,3,4,5)\n",
    "b = a.transpose(3,2,1,0)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.13, pytest-8.3.2, pluggy-1.5.0 -- /Users/longxiangzhang/opt/miniconda3/envs/ml/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2\n",
      "plugins: anyio-4.0.0\n",
      "collected 93 items / 91 deselected / 2 selected                                \u001b[0m\n",
      "\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_relu_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m            [ 50%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_relu_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m91 deselected\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_relu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sequential\n",
    "`needle.nn.Sequential(*modules)`\n",
    "\n",
    "Applies a sequence of modules to the input (in the order that they were passed to the constructor) and returns the output of the last module.\n",
    "These should be kept in a `.module` property: you should _not_ redefine any magic methods like `__getitem__`, as this may not be compatible with our tests.\n",
    "\n",
    "##### Parameters\n",
    "- `*modules` - any number of modules of type `needle.nn.Module`\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.13, pytest-8.3.2, pluggy-1.5.0 -- /Users/longxiangzhang/opt/miniconda3/envs/ml/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2\n",
      "plugins: anyio-4.0.0\n",
      "collected 93 items / 91 deselected / 2 selected                                \u001b[0m\n",
      "\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_sequential_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 50%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_sequential_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m91 deselected\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_sequential\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LogSumExp\n",
    "\n",
    "`needle.ops.LogSumExp(axes)`\n",
    "\n",
    "Here you will need to implement one additional operatior in the `python/needle/ops/ops_logarithmic.py` file, as you did in HW1. Applies a numerically stable log-sum-exp function to the input by subtracting off the maximum elements.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{LogSumExp}(z) = \\log (\\sum_{i} \\exp (z_i - \\max{z})) + \\max{z}\n",
    "\\end{equation}\n",
    "\n",
    "#### Parameters\n",
    "- `axes` - Tuple of axes to sum and take the maximum element over. This uses the same conventions as `needle.ops.Summation()`\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.13, pytest-8.3.2, pluggy-1.5.0 -- /Users/longxiangzhang/opt/miniconda3/envs/ml/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2\n",
      "plugins: anyio-4.0.0\n",
      "collected 93 items / 83 deselected / 10 selected                               \u001b[0m\n",
      "\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 10%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 20%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_3 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 30%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_4 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 40%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_5 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 50%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_1 \u001b[31mFAILED\u001b[0m\u001b[31m      [ 60%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_2 \u001b[31mFAILED\u001b[0m\u001b[31m      [ 70%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_3 \u001b[31mFAILED\u001b[0m\u001b[31m      [ 80%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_5 \u001b[31mFAILED\u001b[0m\u001b[31m      [ 90%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_4 \u001b[31mFAILED\u001b[0m\u001b[31m      [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_1 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           logsumexp_backward((\u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m,)),\u001b[90m\u001b[39;49;00m\n",
      "            np.array([[\u001b[94m1.0\u001b[39;49;00m], [\u001b[94m7.3\u001b[39;49;00m], [\u001b[94m9.9\u001b[39;49;00m]], dtype=np.float32),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:548: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:349: in logsumexp_backward\n",
      "    \u001b[0my.backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x1030cffa0>\n",
      "out_grad = needle.Tensor([1.  7.3 9.9]), node = needle.Tensor([0.5  3.65 4.95])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       Z = node.inputs.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'tuple' object has no attribute 'realize_cached_data'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:45: AttributeError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x1031a4160>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x1031a4040>\n",
      "(3,)\n",
      "<needle.ops.ops_logarithmic.LogSumExp object at 0x1030cffa0>\n",
      "(3,)\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_2 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_2\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           logsumexp_backward((\u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            np.array(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m1.4293308\u001b[39;49;00m, \u001b[94m1.2933122\u001b[39;49;00m, \u001b[94m0.82465225\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.50017685\u001b[39;49;00m, \u001b[94m2.1323113\u001b[39;49;00m, \u001b[94m2.1323113\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m1.4293308\u001b[39;49;00m, \u001b[94m0.58112264\u001b[39;49;00m, \u001b[94m0.40951014\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.3578173\u001b[39;49;00m, \u001b[94m0.07983983\u001b[39;49;00m, \u001b[94m4.359107\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m1.1300558\u001b[39;49;00m, \u001b[94m0.561169\u001b[39;49;00m, \u001b[94m0.1132981\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.9252113\u001b[39;49;00m, \u001b[94m0.65198547\u001b[39;49;00m, \u001b[94m1.7722803\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.2755132\u001b[39;49;00m, \u001b[94m2.365242\u001b[39;49;00m, \u001b[94m2.888913\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.05291228\u001b[39;49;00m, \u001b[94m1.1745441\u001b[39;49;00m, \u001b[94m0.02627547\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m2.748018\u001b[39;49;00m, \u001b[94m0.13681579\u001b[39;49;00m, \u001b[94m2.748018\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                ],\u001b[90m\u001b[39;49;00m\n",
      "                dtype=np.float32,\u001b[90m\u001b[39;49;00m\n",
      "            ),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:557: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:349: in logsumexp_backward\n",
      "    \u001b[0my.backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x101ca0ca0>\n",
      "out_grad = needle.Tensor([10.732058  9.950765 12.416252])\n",
      "node = needle.Tensor([5.366029  4.9753823 6.208126 ])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       Z = node.inputs.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'tuple' object has no attribute 'realize_cached_data'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:45: AttributeError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x101ca0520>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x101ca0070>\n",
      "(3,)\n",
      "<needle.ops.ops_logarithmic.LogSumExp object at 0x101ca0ca0>\n",
      "(3,)\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_3 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_3\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           logsumexp_backward((\u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m), (\u001b[94m0\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            np.array(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.92824626\u001b[39;49;00m, \u001b[94m0.839912\u001b[39;49;00m, \u001b[94m0.5355515\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.59857905\u001b[39;49;00m, \u001b[94m2.551811\u001b[39;49;00m, \u001b[94m2.551811\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m1.0213376\u001b[39;49;00m, \u001b[94m0.41524494\u001b[39;49;00m, \u001b[94m0.29261813\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.16957533\u001b[39;49;00m, \u001b[94m0.03783737\u001b[39;49;00m, \u001b[94m2.0658503\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.98689\u001b[39;49;00m, \u001b[94m0.49007502\u001b[39;49;00m, \u001b[94m0.09894446\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.48244575\u001b[39;49;00m, \u001b[94m0.3399738\u001b[39;49;00m, \u001b[94m0.9241446\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.358991\u001b[39;49;00m, \u001b[94m3.081887\u001b[39;49;00m, \u001b[94m3.764224\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m0.12704718\u001b[39;49;00m, \u001b[94m2.820187\u001b[39;49;00m, \u001b[94m0.06308978\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[94m3.9397335\u001b[39;49;00m, \u001b[94m0.19614778\u001b[39;49;00m, \u001b[94m3.9397335\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                ],\u001b[90m\u001b[39;49;00m\n",
      "                dtype=np.float32,\u001b[90m\u001b[39;49;00m\n",
      "            ),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:585: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:349: in logsumexp_backward\n",
      "    \u001b[0my.backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x103209000>\n",
      "out_grad = needle.Tensor([11.782075 10.288434 11.551383])\n",
      "node = needle.Tensor([5.8910375 5.144217  5.7756915])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       Z = node.inputs.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'tuple' object has no attribute 'realize_cached_data'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:45: AttributeError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x103208d00>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x103208ee0>\n",
      "(3,)\n",
      "<needle.ops.ops_logarithmic.LogSumExp object at 0x103209000>\n",
      "(3,)\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_5 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_5\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        grad_compare = ndl.Tensor(np.array([[\u001b[94m1e10\u001b[39;49;00m, \u001b[94m1e9\u001b[39;49;00m, \u001b[94m1e8\u001b[39;49;00m, -\u001b[94m10\u001b[39;49;00m], [\u001b[94m1e-10\u001b[39;49;00m, \u001b[94m1e9\u001b[39;49;00m, \u001b[94m1e8\u001b[39;49;00m, -\u001b[94m10\u001b[39;49;00m]]))\u001b[90m\u001b[39;49;00m\n",
      ">       test_data = (ndl.ops.logsumexp(grad_compare, (\u001b[94m0\u001b[39;49;00m,)) ** \u001b[94m2\u001b[39;49;00m).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:613: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x10f2122c0>\n",
      "out_grad = needle.Tensor([ 2.00000000e+10  2.00000000e+09  2.00000001e+08 -1.86137056e+01])\n",
      "node = needle.Tensor([ 1.00000000e+10  1.00000000e+09  1.00000001e+08 -9.30685282e+00])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       Z = node.inputs.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'tuple' object has no attribute 'realize_cached_data'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:45: AttributeError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x10f212140>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x10f2121a0>\n",
      "(4,)\n",
      "<needle.ops.ops_logarithmic.LogSumExp object at 0x10f2122c0>\n",
      "(4,)\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_4 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_4\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           logsumexp_backward((\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m), \u001b[94mNone\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "            np.array(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m0.96463485\u001b[39;49;00m, \u001b[94m1.30212122\u001b[39;49;00m, \u001b[94m0.09671321\u001b[39;49;00m, \u001b[94m1.84779774\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m1.84779774\u001b[39;49;00m, \u001b[94m0.39219132\u001b[39;49;00m, \u001b[94m0.21523925\u001b[39;49;00m, \u001b[94m0.30543892\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m0.01952606\u001b[39;49;00m, \u001b[94m0.55654611\u001b[39;49;00m, \u001b[94m0.32109909\u001b[39;49;00m, \u001b[94m0.01598658\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        ],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m1.30212122\u001b[39;49;00m, \u001b[94m0.83026929\u001b[39;49;00m, \u001b[94m0.30543892\u001b[39;49;00m, \u001b[94m0.01680623\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m0.29054249\u001b[39;49;00m, \u001b[94m0.07532032\u001b[39;49;00m, \u001b[94m1.84779774\u001b[39;49;00m, \u001b[94m0.05307731\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m0.75125862\u001b[39;49;00m, \u001b[94m0.26289377\u001b[39;49;00m, \u001b[94m0.04802637\u001b[39;49;00m, \u001b[94m0.03932065\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        ],\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "                ],\u001b[90m\u001b[39;49;00m\n",
      "                dtype=np.float32,\u001b[90m\u001b[39;49;00m\n",
      "            ),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:630: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:349: in logsumexp_backward\n",
      "    \u001b[0my.backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x10f3a7b80>\n",
      "out_grad = needle.Tensor(13.707964897155762), node = needle.Tensor(6.8539824)\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       Z = node.inputs.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'tuple' object has no attribute 'realize_cached_data'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:45: AttributeError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x10f3a7a00>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x10f3a7a90>\n",
      "()\n",
      "<needle.ops.ops_logarithmic.LogSumExp object at 0x10f3a7b80>\n",
      "()\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_1\u001b[0m - AttributeError: 'tuple' object has no attribute 'realize_cached_data'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_2\u001b[0m - AttributeError: 'tuple' object has no attribute 'realize_cached_data'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_3\u001b[0m - AttributeError: 'tuple' object has no attribute 'realize_cached_data'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_5\u001b[0m - AttributeError: 'tuple' object has no attribute 'realize_cached_data'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_4\u001b[0m - AttributeError: 'tuple' object has no attribute 'realize_cached_data'\n",
      "\u001b[31m================== \u001b[31m\u001b[1m5 failed\u001b[0m, \u001b[32m5 passed\u001b[0m, \u001b[33m83 deselected\u001b[0m\u001b[31m in 0.28s\u001b[0m\u001b[31m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_op_logsumexp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"op_logsumexp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SoftmaxLoss\n",
    "\n",
    "`needle.nn.SoftmaxLoss()`\n",
    "\n",
    "Applies the softmax loss as defined below (and as implemented in Homework 1), taking in as input a Tensor of logits and a Tensor of the true labels (expressed as a list of numbers, *not* one-hot encoded).\n",
    "\n",
    "Note that you can use the `init.one_hot` function now instead of writing this yourself.  Note: You will need to use the numerically stable logsumexp operator you just implemented for this purpose.\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell_\\text{softmax}(z,y) = \\log \\sum_{i=1}^k \\exp z_i - z_y\n",
    "\\end{equation}\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.13, pytest-8.3.2, pluggy-1.5.0 -- /Users/longxiangzhang/opt/miniconda3/envs/ml/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2\n",
      "plugins: anyio-4.0.0\n",
      "collected 93 items / 89 deselected / 4 selected                                \u001b[0m\n",
      "\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_softmax_loss_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m    [ 25%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_softmax_loss_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m    [ 50%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_softmax_loss_backward_1 \u001b[31mFAILED\u001b[0m\u001b[31m   [ 75%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_softmax_loss_backward_2 \u001b[31mFAILED\u001b[0m\u001b[31m   [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________ test_nn_softmax_loss_backward_1 ________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_softmax_loss_backward_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           softmax_loss_backward(\u001b[94m5\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "            np.array(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.00068890385\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0015331834\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.013162163\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m0.16422154\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.023983022\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0050903494\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.00076135644\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.050772052\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0062173656\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.062013146\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.012363418\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.02368262\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.11730081\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.001758993\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.004781439\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0029000894\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m0.19815083\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.017544521\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.015874943\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0019439887\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.001219767\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.08134181\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.057320606\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0008595553\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0030001428\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0009499555\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m0.19633561\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0008176346\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0014898272\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0493363\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m0.19886842\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.08767337\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.017700946\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.026406704\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0013147127\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0107361665\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.009714483\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.023893777\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.019562569\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0018656658\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.007933789\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.017656967\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.027691642\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0005605318\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.05576411\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0013114461\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.06811045\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.011835824\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0071787895\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m0.19804356\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                ],\u001b[90m\u001b[39;49;00m\n",
      "                dtype=np.float32,\u001b[90m\u001b[39;49;00m\n",
      "            ),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:997: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:139: in softmax_loss_backward\n",
      "    \u001b[0mloss.backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:263: in gradient\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m broadcast_to(reshape(out_grad, shape), node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:227: in broadcast_to\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:241: in make_from_op\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m tensor.detach()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:273: in detach\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_const(\u001b[96mself\u001b[39;49;00m.realize_cached_data())\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:215: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape)  \u001b[90m# .compact() # Why calling `compact`?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/lib/stride_tricks.py\u001b[0m:413: in broadcast_to\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _broadcast_to(array, shape, subok=subok, readonly=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "array = array([[-0.2, -0.2, -0.2, -0.2, -0.2]], dtype=float32), shape = (5, 10)\n",
      "subok = False, readonly = True\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_broadcast_to\u001b[39;49;00m(array, shape, subok, readonly):\u001b[90m\u001b[39;49;00m\n",
      "        shape = \u001b[96mtuple\u001b[39;49;00m(shape) \u001b[94mif\u001b[39;49;00m np.iterable(shape) \u001b[94melse\u001b[39;49;00m (shape,)\u001b[90m\u001b[39;49;00m\n",
      "        array = np.array(array, copy=\u001b[94mFalse\u001b[39;49;00m, subok=subok)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m shape \u001b[95mand\u001b[39;49;00m array.shape:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcannot broadcast a non-scalar to a scalar array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(size < \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m size \u001b[95min\u001b[39;49;00m shape):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mall elements of broadcast shape must be non-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mnegative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        extras = []\u001b[90m\u001b[39;49;00m\n",
      ">       it = np.nditer(\u001b[90m\u001b[39;49;00m\n",
      "            (array,), flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mmulti_index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrefs_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mzerosize_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + extras,\u001b[90m\u001b[39;49;00m\n",
      "            op_flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mreadonly\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itershape=shape, order=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (1,5)  and requested shape (5,10)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/lib/stride_tricks.py\u001b[0m:349: ValueError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.DivScalar object at 0x115b002e0>\n",
      "()\n",
      "<needle.ops.ops_mathematic.Summation object at 0x115b00040>\n",
      "()\n",
      "[]\n",
      "()\n",
      "0.2\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x115b000d0>\n",
      "(5,)\n",
      "<needle.ops.ops_mathematic.Negate object at 0x115b00100>\n",
      "(5,)\n",
      "<needle.ops.ops_mathematic.Summation object at 0x115b003d0>\n",
      "(5,)\n",
      "[1, 5]\n",
      "(5,)\n",
      "[-0.2 -0.2 -0.2 -0.2 -0.2]\n",
      "\u001b[31m\u001b[1m_______________________ test_nn_softmax_loss_backward_2 ________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_softmax_loss_backward_2\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           softmax_loss_backward(\u001b[94m3\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "            np.array(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0027466794\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.020295369\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.012940894\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.04748398\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.052477922\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.090957515\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0028875037\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.012940894\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.040869843\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.04748398\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m0.33108455\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0063174255\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.001721699\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.09400159\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0034670753\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.038218185\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.009424488\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0042346967\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.08090791\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m0.29697907\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0044518122\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.054234188\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.14326698\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.002624026\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0032049934\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.01176007\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.045363605\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0043262867\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.039044812\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.017543964\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.0037236712\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m0.3119051\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.04104668\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                ],\u001b[90m\u001b[39;49;00m\n",
      "                dtype=np.float32,\u001b[90m\u001b[39;49;00m\n",
      "            ),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1070: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:139: in softmax_loss_backward\n",
      "    \u001b[0mloss.backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:263: in gradient\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m broadcast_to(reshape(out_grad, shape), node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:227: in broadcast_to\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:241: in make_from_op\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m tensor.detach()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:273: in detach\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_const(\u001b[96mself\u001b[39;49;00m.realize_cached_data())\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:215: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape)  \u001b[90m# .compact() # Why calling `compact`?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/lib/stride_tricks.py\u001b[0m:413: in broadcast_to\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _broadcast_to(array, shape, subok=subok, readonly=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "array = array([[-0.33333334, -0.33333334, -0.33333334]], dtype=float32)\n",
      "shape = (3, 11), subok = False, readonly = True\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_broadcast_to\u001b[39;49;00m(array, shape, subok, readonly):\u001b[90m\u001b[39;49;00m\n",
      "        shape = \u001b[96mtuple\u001b[39;49;00m(shape) \u001b[94mif\u001b[39;49;00m np.iterable(shape) \u001b[94melse\u001b[39;49;00m (shape,)\u001b[90m\u001b[39;49;00m\n",
      "        array = np.array(array, copy=\u001b[94mFalse\u001b[39;49;00m, subok=subok)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m shape \u001b[95mand\u001b[39;49;00m array.shape:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcannot broadcast a non-scalar to a scalar array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(size < \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m size \u001b[95min\u001b[39;49;00m shape):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mall elements of broadcast shape must be non-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mnegative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        extras = []\u001b[90m\u001b[39;49;00m\n",
      ">       it = np.nditer(\u001b[90m\u001b[39;49;00m\n",
      "            (array,), flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mmulti_index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrefs_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mzerosize_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + extras,\u001b[90m\u001b[39;49;00m\n",
      "            op_flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mreadonly\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itershape=shape, order=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (1,3)  and requested shape (3,11)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/lib/stride_tricks.py\u001b[0m:349: ValueError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.DivScalar object at 0x115ba5000>\n",
      "()\n",
      "<needle.ops.ops_mathematic.Summation object at 0x115ba4fa0>\n",
      "()\n",
      "[]\n",
      "()\n",
      "0.33333334\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x115ba5210>\n",
      "(3,)\n",
      "<needle.ops.ops_mathematic.Negate object at 0x115ba5240>\n",
      "(3,)\n",
      "<needle.ops.ops_mathematic.Summation object at 0x115ba4a30>\n",
      "(3,)\n",
      "[1, 3]\n",
      "(3,)\n",
      "[-0.33333334 -0.33333334 -0.33333334]\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_nn_softmax_loss_backward_1\u001b[0m - ValueError: operands could not be broadcast together with remapped shapes [...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_nn_softmax_loss_backward_2\u001b[0m - ValueError: operands could not be broadcast together with remapped shapes [...\n",
      "\u001b[31m================== \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m2 passed\u001b[0m, \u001b[33m89 deselected\u001b[0m\u001b[31m in 0.21s\u001b[0m\u001b[31m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_softmax_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_softmax_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LayerNorm1d\n",
    "`needle.nn.LayerNorm1d(dim, eps=1e-5, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies layer normalization over a mini-batch of inputs as described in the paper [Layer Normalization](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "\\begin{equation}\n",
    "y = w \\circ \\frac{x_i - \\textbf{E}[x]}{((\\textbf{Var}[x]+\\epsilon)^{1/2})} + b\n",
    "\\end{equation}\n",
    "\n",
    "where $\\textbf{E}[x]$ denotes the empirical mean of the inputs, $\\textbf{Var}[x]$ denotes their empirical variance (note that here we are using the \"biased\" estimate of the variance, i.e., dividing by $N$ rather than by $N-1$), and $w$ and $b$ denote learnable scalar weights and biases respectively.  Note you can assume the input to this layer is a 2D tensor, with batches in the first dimension and features in the second. You might need to broadcast the weight and bias before applying them.\n",
    "\n",
    "##### Parameters\n",
    "- `dim` - number of channels\n",
    "- `eps` - a value added to the denominator for numerical stability.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of size `dim`, elements initialized to 1.\n",
    "- `bias` - the learnable bias of shape `dim`, elements initialized to 0.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.13, pytest-8.3.2, pluggy-1.5.0 -- /Users/longxiangzhang/opt/miniconda3/envs/ml/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2\n",
      "plugins: anyio-4.0.0\n",
      "collected 93 items / 86 deselected / 7 selected                                \u001b[0m\n",
      "\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 14%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 28%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_forward_3 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 42%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_backward_1 \u001b[31mFAILED\u001b[0m\u001b[31m      [ 57%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_backward_2 \u001b[31mFAILED\u001b[0m\u001b[31m      [ 71%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_backward_3 \u001b[32mPASSED\u001b[0m\u001b[31m      [ 85%]\u001b[0m\n",
      "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_backward_4 \u001b[31mFAILED\u001b[0m\u001b[31m      [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________ test_nn_layernorm_backward_1 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_layernorm_backward_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      "            layernorm_backward((\u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m), \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "            np.array(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    [-\u001b[94m2.8312206e-06\u001b[39;49;00m, -\u001b[94m6.6757202e-05\u001b[39;49;00m, \u001b[94m6.9618225e-05\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[94m1.9950867e-03\u001b[39;49;00m, -\u001b[94m6.8092346e-04\u001b[39;49;00m, -\u001b[94m1.3141632e-03\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[94m4.4703484e-05\u001b[39;49;00m, -\u001b[94m3.2544136e-05\u001b[39;49;00m, -\u001b[94m1.1801720e-05\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                ],\u001b[90m\u001b[39;49;00m\n",
      "                dtype=np.float32,\u001b[90m\u001b[39;49;00m\n",
      "            ),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1188: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "args = (<function assert_allclose.<locals>.compare at 0x106c6b640>, array([[-18.59087 ,  44.645733, -16.825344],\n",
      "       [  9....9950867e-03, -6.8092346e-04, -1.3141632e-03],\n",
      "       [ 4.4703484e-05, -3.2544136e-05, -1.1801720e-05]], dtype=float32))\n",
      "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'verbose': True}\n",
      "\n",
      "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Mismatched elements: 9 / 9 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max absolute difference: 48.730927\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Max relative difference: 6566379.\u001b[0m\n",
      "\u001b[1m\u001b[31mE            x: array([[-18.59087 ,  44.645733, -16.825344],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [  9.31992 ,  -2.262433, -26.902851],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [-20.27291 ,  48.730892, -17.842134]], dtype=float32)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            y: array([[-2.831221e-06, -6.675720e-05,  6.961823e-05],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 1.995087e-03, -6.809235e-04, -1.314163e-03],\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  [ 4.470348e-05, -3.254414e-05, -1.180172e-05]], dtype=float32)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/contextlib.py\u001b[0m:79: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x1099d4f40>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x1099d4e80>\n",
      "(3, 3)\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x1099d4d00>\n",
      "(3, 3)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x1099d4520>\n",
      "(3, 3)\n",
      "(0,)\n",
      "[[-1.1112246e-03 -6.7544050e+00  7.9266677e+00]\n",
      " [ 1.0788596e+01 -4.4235626e-01 -3.0341225e+00]\n",
      " [ 1.0125072e+01 -4.1029215e+00 -1.7795819e-01]]\n",
      "<needle.ops.ops_mathematic.EWiseDiv object at 0x1099d42e0>\n",
      "(3, 3)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x1099d4dc0>\n",
      "(3, 3)\n",
      "(1,)\n",
      "[[-7.0966482e-05 -7.8722749e+00 -9.7447720e+00]\n",
      " [-4.3250225e+01 -6.1150223e-01 -7.9691606e+00]\n",
      " [-1.1283436e+01 -3.3835158e+00 -5.1562574e-02]]\n",
      "<needle.ops.ops_mathematic.Reshape object at 0x1099d43a0>\n",
      "(3, 1)\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x1099d45b0>\n",
      "(3,)\n",
      "<needle.ops.ops_mathematic.AddScalar object at 0x1099d40a0>\n",
      "(3,)\n",
      "<needle.ops.ops_mathematic.DivScalar object at 0x1099d4340>\n",
      "(3,)\n",
      "<needle.ops.ops_mathematic.Summation object at 0x1099d4550>\n",
      "(3,)\n",
      "[1, 3]\n",
      "(3,)\n",
      "[ -2.873786  -24.878595   -2.0059035]\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x1099d4d60>\n",
      "(3, 3)\n",
      "<needle.ops.ops_mathematic.EWiseMul object at 0x1099d4790>\n",
      "(3, 3)\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x1099d4580>\n",
      "(3, 3)\n",
      "<needle.ops.ops_mathematic.Negate object at 0x1099d40d0>\n",
      "(3, 3)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x1099d4a90>\n",
      "(3, 3)\n",
      "(1,)\n",
      "[[ -0.38208488 -53.92706     -2.6097221 ]\n",
      " [-28.292875    -7.018894     7.4677854 ]\n",
      " [  1.2999535  -58.01222     -1.592932  ]]\n",
      "<needle.ops.ops_mathematic.Reshape object at 0x1099d42b0>\n",
      "(3, 1)\n",
      "<needle.ops.ops_mathematic.DivScalar object at 0x1099d46d0>\n",
      "(3,)\n",
      "<needle.ops.ops_mathematic.Summation object at 0x106c91d20>\n",
      "(3,)\n",
      "[1, 3]\n",
      "(3,)\n",
      "[-18.972956  -9.281327 -19.435066]\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x1099d4250>\n",
      "(3, 3)\n",
      "(0,)\n",
      "[[7.2507435e-05 8.0432110e+00 9.9563665e+00]\n",
      " [1.5017579e+01 2.1232913e-01 2.7670956e+00]\n",
      " [1.3798888e+01 4.1378140e+00 6.3057579e-02]]\n",
      "\u001b[31m\u001b[1m_________________________ test_nn_layernorm_backward_2 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_layernorm_backward_2\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           layernorm_backward((\u001b[94m2\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m), \u001b[94m10\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "            np.array(\u001b[90m\u001b[39;49;00m\n",
      "                [\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m2.301574\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m4.353944\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m1.9396116\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m2.4330146\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m1.1070801\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.01571643\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m2.209449\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0.49513134\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m2.261348\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m2.5212562\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m9.042961\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m2.6184766\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m4.5592957\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m4.2109876\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m3.4247458\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m1.9075732\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        -\u001b[94m2.2689414\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m2.110825\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m5.044025\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m4.910048\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    ],\u001b[90m\u001b[39;49;00m\n",
      "                ],\u001b[90m\u001b[39;49;00m\n",
      "                dtype=np.float32,\u001b[90m\u001b[39;49;00m\n",
      "            ),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1205: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:113: in layernorm_backward\n",
      "    \u001b[0m(f(x) ** \u001b[94m4\u001b[39;49;00m).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:263: in gradient\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m broadcast_to(reshape(out_grad, shape), node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:227: in broadcast_to\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:215: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape)  \u001b[90m# .compact() # Why calling `compact`?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/lib/stride_tricks.py\u001b[0m:413: in broadcast_to\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _broadcast_to(array, shape, subok=subok, readonly=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "array = array([[-2.012053, -4.344184]], dtype=float32), shape = (2, 10)\n",
      "subok = False, readonly = True\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_broadcast_to\u001b[39;49;00m(array, shape, subok, readonly):\u001b[90m\u001b[39;49;00m\n",
      "        shape = \u001b[96mtuple\u001b[39;49;00m(shape) \u001b[94mif\u001b[39;49;00m np.iterable(shape) \u001b[94melse\u001b[39;49;00m (shape,)\u001b[90m\u001b[39;49;00m\n",
      "        array = np.array(array, copy=\u001b[94mFalse\u001b[39;49;00m, subok=subok)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m shape \u001b[95mand\u001b[39;49;00m array.shape:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcannot broadcast a non-scalar to a scalar array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(size < \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m size \u001b[95min\u001b[39;49;00m shape):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mall elements of broadcast shape must be non-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mnegative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        extras = []\u001b[90m\u001b[39;49;00m\n",
      ">       it = np.nditer(\u001b[90m\u001b[39;49;00m\n",
      "            (array,), flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mmulti_index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrefs_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mzerosize_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + extras,\u001b[90m\u001b[39;49;00m\n",
      "            op_flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mreadonly\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itershape=shape, order=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (1,2)  and requested shape (2,10)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/lib/stride_tricks.py\u001b[0m:349: ValueError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x109c1ed70>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x109c1ed10>\n",
      "(2, 10)\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x109c1ec80>\n",
      "(2, 10)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x109c1ee30>\n",
      "(2, 10)\n",
      "(0,)\n",
      "[[ 2.2854114e+00  1.6840569e+01 -1.4186844e+01 -2.6313353e-01\n",
      "   1.1517207e-01  2.2242058e-03  1.0628963e+00 -9.5841665e+00\n",
      "   2.6083710e+00 -4.9897933e+00]\n",
      " [-2.4160233e+01  2.3907353e-01 -8.0687457e-01  2.4912384e+00\n",
      "  -2.2069208e-01  8.1076775e+00  1.5657508e-01  1.4124281e+01\n",
      "  -2.8690560e+00 -1.3104055e+00]]\n",
      "<needle.ops.ops_mathematic.EWiseDiv object at 0x109c1eec0>\n",
      "(2, 10)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x109c1f430>\n",
      "(2, 10)\n",
      "(1,)\n",
      "[[-1.4176780e+00 -2.0328285e+01 -1.6173578e+01 -7.9407223e-02\n",
      "  -2.6388904e-02 -1.3672828e-04 -5.1083374e-01 -9.5873270e+00\n",
      "  -1.6908987e+00 -4.0154591e+00]\n",
      " [-4.5271183e+01 -9.6175030e-02 -4.8688728e-01 -2.1889746e+00\n",
      "  -8.6444236e-02 -1.0557206e+01 -5.4699566e-02 -2.2129698e+01\n",
      "  -2.6424437e+00 -9.2945415e-01]]\n",
      "<needle.ops.ops_mathematic.Reshape object at 0x109c1f2e0>\n",
      "(2, 1)\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x109c1f130>\n",
      "(2,)\n",
      "<needle.ops.ops_mathematic.AddScalar object at 0x109c1f190>\n",
      "(2,)\n",
      "<needle.ops.ops_mathematic.DivScalar object at 0x109c1f460>\n",
      "(2,)\n",
      "<needle.ops.ops_mathematic.Summation object at 0x109c1f4f0>\n",
      "(2,)\n",
      "[1, 2]\n",
      "(2,)\n",
      "[-2.012053 -4.344184]\n",
      "\u001b[31m\u001b[1m_________________________ test_nn_layernorm_backward_4 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_layernorm_backward_4\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           layernorm_backward((\u001b[94m5\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), \u001b[94m1\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "            np.array([[\u001b[94m0\u001b[39;49;00m], [\u001b[94m0\u001b[39;49;00m], [\u001b[94m0\u001b[39;49;00m], [\u001b[94m0\u001b[39;49;00m], [\u001b[94m0\u001b[39;49;00m]], dtype=np.float32),\u001b[90m\u001b[39;49;00m\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1253: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:113: in layernorm_backward\n",
      "    \u001b[0m(f(x) ** \u001b[94m4\u001b[39;49;00m).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:297: in backward\n",
      "    \u001b[0mcompute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:390: in compute_gradient_of_variables\n",
      "    \u001b[0minput_grads = node.op.gradient_as_tuple(gard, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:263: in gradient\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m broadcast_to(reshape(out_grad, shape), node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:227: in broadcast_to\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:215: in compute\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape)  \u001b[90m# .compact() # Why calling `compact`?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/lib/stride_tricks.py\u001b[0m:413: in broadcast_to\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _broadcast_to(array, shape, subok=subok, readonly=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "array = array([[0., 0., 0., 0., 0.]], dtype=float32), shape = (5, 1)\n",
      "subok = False, readonly = True\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_broadcast_to\u001b[39;49;00m(array, shape, subok, readonly):\u001b[90m\u001b[39;49;00m\n",
      "        shape = \u001b[96mtuple\u001b[39;49;00m(shape) \u001b[94mif\u001b[39;49;00m np.iterable(shape) \u001b[94melse\u001b[39;49;00m (shape,)\u001b[90m\u001b[39;49;00m\n",
      "        array = np.array(array, copy=\u001b[94mFalse\u001b[39;49;00m, subok=subok)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m shape \u001b[95mand\u001b[39;49;00m array.shape:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcannot broadcast a non-scalar to a scalar array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(size < \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m size \u001b[95min\u001b[39;49;00m shape):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mall elements of broadcast shape must be non-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mnegative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        extras = []\u001b[90m\u001b[39;49;00m\n",
      ">       it = np.nditer(\u001b[90m\u001b[39;49;00m\n",
      "            (array,), flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mmulti_index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrefs_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mzerosize_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + extras,\u001b[90m\u001b[39;49;00m\n",
      "            op_flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mreadonly\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itershape=shape, order=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (1,5)  and requested shape (5,1)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/site-packages/numpy/lib/stride_tricks.py\u001b[0m:349: ValueError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "<needle.ops.ops_mathematic.Summation object at 0x109a4c0a0>\n",
      "()\n",
      "[]\n",
      "()\n",
      "1.0\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x109a4cca0>\n",
      "(5, 1)\n",
      "<needle.ops.ops_mathematic.EWiseAdd object at 0x109a4cc40>\n",
      "(5, 1)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x109a4ebc0>\n",
      "(5, 1)\n",
      "(0,)\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "<needle.ops.ops_mathematic.EWiseDiv object at 0x109a4ec20>\n",
      "(5, 1)\n",
      "<needle.ops.ops_mathematic.BroadcastTo object at 0x109a4e980>\n",
      "(5, 1)\n",
      "()\n",
      "[[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "<needle.ops.ops_mathematic.Reshape object at 0x109a4e920>\n",
      "(5, 1)\n",
      "<needle.ops.ops_mathematic.PowerScalar object at 0x109a4cd90>\n",
      "(5,)\n",
      "<needle.ops.ops_mathematic.AddScalar object at 0x109a4cdc0>\n",
      "(5,)\n",
      "<needle.ops.ops_mathematic.DivScalar object at 0x109a4cc10>\n",
      "(5,)\n",
      "<needle.ops.ops_mathematic.Summation object at 0x109a4e5c0>\n",
      "(5,)\n",
      "[1, 5]\n",
      "(5,)\n",
      "[0. 0. 0. 0. 0.]\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_nn_layernorm_backward_1\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_nn_layernorm_backward_2\u001b[0m - ValueError: operands could not be broadcast together with remapped shapes [...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_nn_layernorm_backward_4\u001b[0m - ValueError: operands could not be broadcast together with remapped shapes [...\n",
      "\u001b[31m================== \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m4 passed\u001b[0m, \u001b[33m86 deselected\u001b[0m\u001b[31m in 0.23s\u001b[0m\u001b[31m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_layernorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_layernorm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Flatten\n",
    "`needle.nn.Flatten()`\n",
    "\n",
    "Takes in a tensor of shape `(B,X_0,X_1,...)`, and flattens all non-batch dimensions so that the output is of shape `(B, X_0 * X_1 * ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_flatten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_flatten\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm1d\n",
    "`needle.nn.BatchNorm1d(dim, eps=1e-5, momentum=0.1, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies batch normalization over a mini-batch of inputs as described in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167).\n",
    "\n",
    "\\begin{equation}\n",
    "y = w \\circ \\frac{z_i - \\textbf{E}[x]}{((\\textbf{Var}[x]+\\epsilon)^{1/2})} + b\n",
    "\\end{equation}\n",
    "\n",
    "but where here the mean and variance refer to to the mean and variance over the _batch_dimensions.  The function also computes a running average of mean/variance for all features at each layer $\\hat{\\mu}, \\hat{\\sigma}^2$, and at test time normalizes by these quantities:\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\frac{(x - \\hat{mu})}{((\\hat{\\sigma}^2_{i+1})_j+\\epsilon)^{1/2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "BatchNorm uses the running estimates of mean and variance instead of batch statistics at test time, i.e.,\n",
    "after `model.eval()` has been called on the BatchNorm layer's `training` flag is false.\n",
    "\n",
    "To compute the running estimates, you can use the equation $$\\hat{x_{new}} = (1 - m) \\hat{x_{old}} + mx_{observed},$$\n",
    "where $m$ is momentum.\n",
    "\n",
    "##### Parameters\n",
    "- `dim` - input dimension\n",
    "- `eps` - a value added to the denominator for numerical stability.\n",
    "- `momentum` - the value used for the running mean and running variance computation.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of size `dim`, elements initialized to 1.\n",
    "- `bias` - the learnable bias of size `dim`, elements initialized to 0.\n",
    "- `running_mean` - the running mean used at evaluation time, elements initialized to 0.\n",
    "- `running_var` - the running (unbiased) variance used at evaluation time, elements initialized to 1. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.13, pytest-8.3.2, pluggy-1.5.0 -- /Users/longxiangzhang/opt/miniconda3/envs/ml/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2\n",
      "plugins: anyio-4.0.0\n",
      "collected 0 items / 2 errors                                                   \u001b[0m\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m___________________ ERROR collecting tests/hw2/test_data.py ____________________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2/tests/hw2/test_data.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/hw2/test_data.py\u001b[0m:7: in <module>\n",
      "    \u001b[0m\u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mmugrade\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   ModuleNotFoundError: No module named 'mugrade'\u001b[0m\u001b[0m\n",
      "\u001b[31m\u001b[1m_______________ ERROR collecting tests/hw2/test_nn_and_optim.py ________________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/Users/longxiangzhang/Documents/GitHub/dlsyscourse/DLS-hw2/tests/hw2/test_nn_and_optim.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "\u001b[1m\u001b[31m../../../../opt/miniconda3/envs/ml/lib/python3.10/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:11: in <module>\n",
      "    \u001b[0m\u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mmugrade\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   ModuleNotFoundError: No module named 'mugrade'\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/hw2/test_data.py\n",
      "\u001b[31mERROR\u001b[0m tests/hw2/test_nn_and_optim.py\n",
      "!!!!!!!!!!!!!!!!!!! Interrupted: 2 errors during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m2 errors\u001b[0m\u001b[31m in 0.28s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_batchnorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_batchnorm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "`needle.nn.Dropout(p = 0.5)`\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper [Improving neural networks by preventing co-adaption of feature detectors](https://arxiv.org/abs/1207.0580). During evaluation the module simply computes an identity function. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{z}_{i+1} = \\sigma_i (W_i^T z_i + b_i) \\\\\n",
    "(z_{i+1})_j = \n",
    "    \\begin{cases}\n",
    "    (\\hat{z}_{i+1})_j /(1-p) & \\text{with probability } 1-p \\\\\n",
    "    0 & \\text{with probability } p \\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "**Important**: If the Dropout module the flag `training=False`, you shouldn't \"dropout\" any weights. That is, dropout applies during training only, not during evaluation. Note that `training` is a flag in `nn.Module`.\n",
    "\n",
    "##### Parameters\n",
    "- `p` - the probability of an element to be zeroed.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_dropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Residual\n",
    "`needle.nn.Residual(fn: Module)`\n",
    "\n",
    "Applies a residual or skip connection given module $\\mathcal{F}$ and input Tensor $x$, returning $\\mathcal{F}(x) + x$.\n",
    "##### Parameters\n",
    "- `fn` - module of type `needle.nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_residual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_residual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Implement the `step` function of the following optimizers in `python/needle/optim.py`.\n",
    "Make sure that your optimizers _don't_ modify the gradients of tensors in-place.\n",
    "\n",
    "We have included some tests to ensure that you are not consuming excessive memory, which can happen if you are\n",
    "not using `.data` or `.detach()` in the right places, thus building an increasingly large computational graph\n",
    "(not just in the optimizers, but in the previous modules as well).\n",
    "You can ignore these tests, which include the string `memory_check` at your own discretion.\n",
    "\n",
    "___\n",
    "\n",
    "### SGD\n",
    "`needle.optim.SGD(params, lr=0.01, momentum=0.0, weight_decay=0.0)`\n",
    "\n",
    "Implements stochastic gradient descent (optionally with momentum, shown as $\\beta$ below). \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    u_{t+1} &= \\beta u_t + (1-\\beta) \\nabla_\\theta f(\\theta_t) \\\\\n",
    "    \\theta_{t+1} &= \\theta_t - \\alpha u_{t+1}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "##### Parameters\n",
    "- `params` - iterable of parameters of type `needle.nn.Parameter` to optimize\n",
    "- `lr` (*float*) - learning rate\n",
    "- `momentum` (*float*) - momentum factor\n",
    "- `weight_decay` (*float*) - weight decay (L2 penalty)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_optim_sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"optim_sgd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adam\n",
    "`needle.optim.Adam(params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0)`\n",
    "\n",
    "Implements Adam algorithm, proposed in [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "u_{t+1} &= \\beta_1 u_t + (1-\\beta_1) \\nabla_\\theta f(\\theta_t) \\\\\n",
    "v_{t+1} &= \\beta_2 v_t + (1-\\beta_2) (\\nabla_\\theta f(\\theta_t))^2 \\\\\n",
    "\\hat{u}_{t+1} &= u_{t+1} / (1 - \\beta_1^t) \\quad \\text{(bias correction)} \\\\\n",
    "\\hat{v}_{t+1} &= v_{t+1} / (1 - \\beta_2^t) \\quad \\text{(bias correction)}\\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha \\hat{u_{t+1}}/(\\hat{v}_{t+1}^{1/2}+\\epsilon)\n",
    "\\end{split}\n",
    "    \\end{equation}\n",
    "\n",
    "**Important:** Pay attention to whether or not you are applying bias correction.\n",
    "\n",
    "##### Parameters\n",
    "- `params` - iterable of parameters of type `needle.nn.Parameter` to optimize\n",
    "- `lr` (*float*) - learning rate\n",
    "- `beta1` (*float*) - coefficient used for computing running average of gradient\n",
    "- `beta2` (*float*) - coefficient used for computing running average of square of gradient\n",
    "- `eps` (*float*) - term added to the denominator to improve numerical stability\n",
    "- `weight_decay` (*float*) - weight decay (L2 penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_optim_adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"optim_adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "In this question, you will implement two data primitives: `needle.data.DataLoader` and `needle.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples. \n",
    "\n",
    "For this question, you will be working in the `python/needle/data` directory. \n",
    "\n",
    "### Transformations\n",
    "\n",
    "First we will implement a few transformations that are helpful when working with images. We will stick with a horizontal flip and a random crop for now. Fill out the following functions in `needle/data/data_transforms.py`.\n",
    "___ \n",
    "\n",
    "#### RandomFlipHorizontal\n",
    "`needle.data.RandomFlipHorizontal(p = 0.5)`\n",
    "\n",
    "Flips the image horizontally, with probability `p`.\n",
    "\n",
    "##### Parameters\n",
    "- `p` (*float*) - The probability of flipping the input image.\n",
    "___\n",
    "\n",
    "#### RandomCrop\n",
    "`needle.data.RandomCrop(padding=3)`\n",
    "\n",
    "Padding is added to all sides of the image, and then the image is cropped back to it's original size at a random location. Returns an image the same size as the original image.\n",
    "\n",
    "##### Parameters\n",
    "- `padding` (*int*) - The padding on each border of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"flip_horizontal\"\n",
    "!python3 -m pytest -v -k \"random_crop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"flip_horizontal\"\n",
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"random_crop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Each `Dataset` subclass must implement three functions: `__init__`, `__len__`, and `__getitem__`. The `__init__` function initializes the images, labels, and transforms. The `__len__` function returns the number of samples in the dataset. The `__getitem__` function retrieves a sample from the dataset at a given index `idx`, calls the transform functions on the image (if applicable), converts the image and label to a numpy array (the data will be converted to Tensors elsewhere). The output of `__getitem__` and `__next__` should be NDArrays, and you should follow the shapes such that you're accessing an array of size (Datapoint Number, Feature Dim 1, Feature Dim 2, ...). \n",
    "\n",
    "Fill out these functions in the `MNISTDataset` class in `needle/data/datasets/mnist_dataset.py`. You can use your solution to `parse_mnist` from the previous homework for the `__init__` function.\n",
    "\n",
    "### MNISTDataset\n",
    "`needle.data.MNISTDataset(image_filesname, label_filesname, transforms)`\n",
    "\n",
    "##### Parameters\n",
    "- `image_filesname` - path of file containing images\n",
    "- `label_filesname` - path of file containing labels\n",
    "- `transforms` - an optional list of transforms to apply to data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_mnist_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"mnist_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "In `needle/data/data_basic.py`, the Dataloader class provides an interface for assembling mini-batches of examples suitable for training using SGD-based approaches, backed by a Dataset object.  In order to build the typical Dataloader interface (allowing users to iterate over all the mini-batches in the dataset), you will need to implement the `__iter__()` and `__next__()` calls in the class: `__iter__()` is called at the start of iteration, while `__next__()` is called to grab the next mini-batch. Please note that subsequent calls to next will require you to return the following batches, so next is not a pure function.\n",
    "\n",
    "### Dataloader\n",
    "`needle.data.Dataloader(dataset: Dataset, batch_size: Optional[int] = 1, shuffle: bool = False)`\n",
    "\n",
    "Combines a dataset and a sampler, and provides an iterable over the given dataset. \n",
    "\n",
    "##### Parameters\n",
    "- `dataset` - `needle.data.Dataset` - a dataset \n",
    "- `batch_size` - `int` - what batch size to serve the data in \n",
    "- `shuffle` - `bool` - set to ``True`` to have the data reshuffle at every epoch, default ``False``.\n",
    "___ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_dataloader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"dataloader\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Given you have now implemented all the necessary components for our neural network library, let's build and train an MLP ResNet. For this question, you will be working in `apps/mlp_resnet.py`. First, fill out the functions `ResidualBlock` and `MLPResNet` as described below:\n",
    "\n",
    "### ResidualBlock\n",
    "`ResidualBlock(dim, hidden_dim, norm=nn.BatchNorm1d, drop_prob=0.1)`\n",
    "\n",
    "Implements a residual block as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.ibb.co/JpnP5nv/residualblock.png\" alt=\"Residual Block\"/>\n",
    "</p>\n",
    "\n",
    "**NOTE**: if the figure does not render, please see the figure in the `figures` directory.\n",
    "\n",
    "where the first linear layer has `in_features=dim` and `out_features=hidden_dim`, and the last linear layer has `out_features=dim`. Returns the block as type `nn.Module`. \n",
    "\n",
    "##### Parameters\n",
    "- `dim` (*int*) - input dim\n",
    "- `hidden_dim` (*int*) - hidden dim\n",
    "- `norm` (*nn.Module*) - normalization method\n",
    "- `drop_prob` (*float*) - dropout probability\n",
    "\n",
    "___\n",
    "\n",
    "### MLPResNet\n",
    "`MLPResNet(dim, hidden_dim=100, num_blocks=3, num_classes=10, norm=nn.BatchNorm1d, drop_prob=0.1)`\n",
    "\n",
    "Implements an MLP ResNet as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.ibb.co/4WmvkVS/mlp-resnet.png\" alt=\"MLP Resnet\"/>\n",
    "</p>\n",
    "\n",
    "where the first linear layer has `in_features=dim` and `out_features=hidden_dim`, and each ResidualBlock has `dim=hidden_dim` and `hidden_dim=hidden_dim//2`. Returns a network of type `nn.Module`.\n",
    "\n",
    "##### Parameters\n",
    "- `dim` (*int*) - input dim\n",
    "- `hidden_dim` (*int*) - hidden dim\n",
    "- `num_blocks` (*int*) - number of ResidualBlocks\n",
    "- `num_classes` (*int*) - number of classes\n",
    "- `norm` (*nn.Module*) - normalization method\n",
    "- `drop_prob` (*float*) - dropout probability (0.1)\n",
    "___ \n",
    "\n",
    "Once you have the deep learning model architecture correct, let's train the network using our new neural network library components. Specifically, implement the functions `epoch` and `train_mnist`.\n",
    "\n",
    "### Epoch\n",
    "\n",
    "`epoch(dataloader, model, opt=None)`\n",
    "\n",
    "Executes one epoch of training or evaluation, iterating over the entire training dataset once (just like `nn_epoch` from previous homeworks). Returns the average error rate (as a *float*) and the average loss over all samples (as a *float*). Set the model to `training` mode at the beginning of the function if `opt` is given; set the model to `eval` if `opt` is not given (i.e. `None`). When setting the modes, use `.train()` and `.eval()` instead of modifying the training attribute.\n",
    "\n",
    "##### Parameters\n",
    "- `dataloader` (*`needle.data.DataLoader`*) - dataloader returning samples from the training dataset\n",
    "- `model` (*`needle.nn.Module`*) - neural network\n",
    "- `opt` (*`needle.optim.Optimizer`*) - optimizer instance, or `None`\n",
    "\n",
    "___\n",
    "\n",
    "### Train Mnist\n",
    "\n",
    "`train_mnist(batch_size=100, epochs=10, optimizer=ndl.optim.Adam, lr=0.001, weight_decay=0.001, hidden_dim=100, data_dir=\"data\")`\n",
    "                \n",
    "Initializes a training dataloader (with `shuffle` set to `True`) and a test dataloader for MNIST data, and trains an `MLPResNet` using the given optimizer (if `opt` is not None) and the softmax loss for a given number of epochs. Returns a tuple of the training accuracy, training loss, test accuracy, test loss computed in the last epoch of training. If any parameters are not specified, use the default parameters.\n",
    "\n",
    "##### Parameters\n",
    "- `batch_size` (*int*) - batch size to use for train and test dataloader\n",
    "- `epochs` (*int*) - number of epochs to train for\n",
    "- `optimizer` (*`needle.optim.Optimizer` type*) - optimizer type to use\n",
    "- `lr` (*float*) - learning rate \n",
    "- `weight_decay` (*float*) - weight decay\n",
    "- `hidden_dim` (*int*) - hidden dim for `MLPResNet`\n",
    "- `data_dir` (*int*) - directory containing MNIST image/label files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_mlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"mlp_resnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage to experiment with the `mlp_resnet.py` training script.\n",
    "You can investigate the effect of using different initializers on the Linear layers,\n",
    "increasing the dropout probability,\n",
    "or adding transforms (via a list to the `transforms=` keyword argument of Dataset)\n",
    "such as random cropping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
